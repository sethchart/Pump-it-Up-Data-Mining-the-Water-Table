{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "In this notebook, I will tell you about building my submission to the [Pump it Up: Data Mining the Water Table](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/) competition hosted by [DrivenData](https://www.drivendata.org/). The competition problem uses data about waterpoints (pumps, wells, etc.) in Tanzania collected by a partnership between [Taarifa](http://taarifa.org/) and the [United Republic of Tanzania Ministry of Water](https://www.maji.go.tz/) to predict if they are currently in need of repair. By better understanding what factors predict issues with water points, we hope to improve access to potable water in Tanzania and reduce operational costs associated with maintaining water infrastructure. \n",
    "\n",
    "For the purposes of the competition, the model is assessed in terms of the classification rate, also commonly called [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision).The classification rate is a number between zero and one with zero indicating that all prediction were wrong, one indicating all predictions were correct, and 0.7 indicating that, on average, seven out of ten predictions are correct.\n",
    "\n",
    "All relevant data and code is available in the project [github repository](https://github.com/sethchart/Pump-it-Up-Data-Mining-the-Water-Table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Modules\n",
    "Below I collect the tools that I will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utilities import DataWrapper\n",
    "#Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "#Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer,FunctionTransformer\n",
    "\n",
    "#Resampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Classifiers\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataWrapper()\n",
    "X_train = data.X_train\n",
    "X_test = data.X_test\n",
    "y_train = data.y_train\n",
    "y_test = data.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions From Data Inspection\n",
    "After inspecting the data, there are some issues that need to be addressed before we proceed to modeling. \n",
    "* There are several variables the explicitly depend on other variables, these should be dropped from the model to avoid issues. There are two reasonable strategies for dealing with these:\n",
    "    * Keep the variable with the most granular data. There is no information loss associated with this approach, but it will increase model complexity.\n",
    "    * Keep the variable with the coarser data, thereby aggregating the granular information in the finer variable. This approach reduces model complexity, but it also results in some loss of information.\n",
    "* There are some variables with a huge number of values. Including these variables will greatly increase the complexity of our model and the time required to train the model.\n",
    "* The Data Collection and Unknown variable groups should be excluded because any relevant information that they carry is from some unknown lurking variables.\n",
    "* We will try to deal with all missing values, data encoding, and distributional issues with standard data cleaning procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Columns to Drop from the Model\n",
    "In this section I will identify columns that should be dropped from the model. Below I have initialized the `drop_cols` list with the columns that will be dropped regardless of my approach to the other issues identified in the conclusion of the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'date_recorded', \n",
    "    'recorded_by',\n",
    "    'num_private',\n",
    "    'public_meeting'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Variables\n",
    "We will need to pre-process our data in preparation for classification. Pre-processing is different for categorical and numerical variables. In order to implement different pre-pricessing flows, we must first classify all of our variables as categorical or numerical. The function below separates columns into these two classes and excludes any variables that will be dropped from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_columns(drop_cols, df=X_train):\n",
    "    \"\"\"Takes a dataframe and a list of columns to drop and returns:\n",
    "        - cat_cols: A list of categorical columns.\n",
    "        - num_cols: A list of numerical columns.\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    keep_cols = [col for col in cols if col not in drop_cols]\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    for col in keep_cols:\n",
    "        if df[col].dtype == object:\n",
    "            cat_cols.append(col)\n",
    "        else:\n",
    "            num_cols.append(col)\n",
    "    return cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Categorical Variables With Many Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_complex_categorical_variables(drop_cols, df=X_train, threshold= 20):\n",
    "    cat_cols, num_cols = classify_columns(drop_cols)\n",
    "    for col in cat_cols:\n",
    "        if len(df[col].unique()) > threshold:\n",
    "            drop_cols.append(col)\n",
    "    return drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_recorded',\n",
       " 'recorded_by',\n",
       " 'num_private',\n",
       " 'public_meeting',\n",
       " 'region',\n",
       " 'funder',\n",
       " 'installer',\n",
       " 'wpt_name',\n",
       " 'subvillage',\n",
       " 'lga',\n",
       " 'ward',\n",
       " 'scheme_name']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_complex_categorical_variables(drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Dependent Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will check for pairwise functional dependence between variables and drop variables according to the selected strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dependent_categorical_varables(drop_cols, df=X_train, priority='information'):\n",
    "    cat_cols, num_cols = classify_columns(drop_cols)\n",
    "    N = len(cat_cols)\n",
    "    cat_combs = combinations(cat_cols, 2)\n",
    "    for comb in tqdm(cat_combs):\n",
    "        dict1 = {}\n",
    "        dict2 = {}\n",
    "        col1 = comb[0]\n",
    "        col2 = comb[1]\n",
    "        col_dict = {}\n",
    "        for pair in set(zip(df[col1], df[col2])):\n",
    "            dict1[pair[0]] = dict1.get(pair[0],0)+1\n",
    "            dict2[pair[1]] = dict2.get(pair[1],0)+1\n",
    "        if max(dict1.values()) == 1 and max(dict2.values()) == 1:\n",
    "            col_dict['fine'] = col1\n",
    "            col_dict['coarse'] = col2\n",
    "        elif max(dict1.values()) == 1:\n",
    "            col_dict['fine'] = col1\n",
    "            col_dict['coarse'] = col2\n",
    "        elif max(dict2.values()) == 1:\n",
    "            col_dict['fine'] = col2\n",
    "            col_dict['coarse'] = col1\n",
    "        else:\n",
    "            continue\n",
    "        if priority == 'information':\n",
    "            drop_cols.append(col_dict['coarse'])\n",
    "        elif priority == 'complexity':\n",
    "            drop_cols.append(col_dict['fine'])\n",
    "        else:\n",
    "            pass\n",
    "    return drop_cols    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26307c6f32584a51ab12d4d350e08c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['date_recorded',\n",
       " 'recorded_by',\n",
       " 'num_private',\n",
       " 'public_meeting',\n",
       " 'region',\n",
       " 'funder',\n",
       " 'installer',\n",
       " 'wpt_name',\n",
       " 'subvillage',\n",
       " 'lga',\n",
       " 'ward',\n",
       " 'scheme_name',\n",
       " 'extraction_type_group',\n",
       " 'extraction_type_class',\n",
       " 'extraction_type_class',\n",
       " 'management_group',\n",
       " 'payment_type',\n",
       " 'quality_group',\n",
       " 'quantity_group',\n",
       " 'source_type',\n",
       " 'source_class',\n",
       " 'source_class',\n",
       " 'waterpoint_type_group']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_dependent_categorical_varables(drop_cols, priority='information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Approach\n",
    "\n",
    "Initial experiments with unoptimized models showed that the [XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/index.html) performed best on this data. With that in mind, I want to build an [sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to implement data preprocessing and classification. To optimize the model I will use an [sklearn grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to select the best hyper-parameters for the model and perform five-fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "We experimented with both manual and automated feature selection, however neither approach improved model performance. Initially, we has issues with mixed data types in both the `public_meeting` and `permit` columns. The function below converts all categorical variables to strings to eliminate thoes errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basin', 'scheme_management', 'permit', 'extraction_type', 'management', 'payment', 'water_quality', 'quantity', 'source', 'waterpoint_type']\n",
      "['amount_tsh', 'gps_height', 'longitude', 'latitude', 'region_code', 'district_code', 'population', 'construction_year']\n"
     ]
    }
   ],
   "source": [
    "cat_cols, num_cols = classify_columns(drop_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Preprocessors\n",
    "Below we build a preprocessing step for our pipeline which handles all data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Pipeline\n",
    "The pipeline below executes the following three steps for all of our categorical data.\n",
    "1. Convert all values in categorical columns to strings. This avoids data type errors in the following steps.\n",
    "2. Fill all missing values with the string `missing`.\n",
    "3. One-hot encode all categorical variables. Because this data contains categorical variables with many possible values, it is possible to encounter values in testing data that was not present in the training data. For this reason, we need to set `handel_unknown` to `ignore` so that the encoder will simply ignore unknown values in testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical_to_string(data):\n",
    "    return pd.DataFrame(data).astype(str)\n",
    "\n",
    "CategoricalTypeConverter = FunctionTransformer(convert_categorical_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('typeConverter', CategoricalTypeConverter),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('standardizer', OneHotEncoder(handle_unknown='ignore',dtype=float))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Pipeline\n",
    "The pipeline below executes two steps:\n",
    "1. Imputes missing values in any numerical column with the median value from that column.\n",
    "2. Scales each variable to have mean zero and standard deviation one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('standardizer', PowerTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessors\n",
    "The column transformer below implements each of the three possible pre-processing behaviors. \n",
    "1. Apply the categorical pipeline.\n",
    "2. Apply the numerical pipeline.\n",
    "3. Drop the specified columns.\n",
    "The if-then statement below ensures that the drop processor is only implemented if there are columns to drop. This is needed since passing an empty `drop_col` list throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessor(drop_cols=None, complexity_threshold=None, priority=None):\n",
    "    if drop_cols==None:\n",
    "        drop_cols = []\n",
    "    if complexity_threshold != None:\n",
    "        drop_cols = drop_complex_categorical_variables(drop_cols, threshold=complexity_threshold)\n",
    "    if priority != None:\n",
    "        drop_cols = drop_dependent_categorical_varables(drop_cols, priority=priority)\n",
    "    cat_cols, num_cols = classify_columns(drop_cols)    \n",
    "    if len(drop_cols) > 0:\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numericalPreprocessor', numerical_pipeline, num_cols),\n",
    "            ('categoricalPreprocessor', categorical_pipeline, cat_cols),\n",
    "            ('dropPreprocessor', 'drop', drop_cols)\n",
    "        ])\n",
    "    else:\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numericalPreprocessor', numerical_pipeline, num_cols),\n",
    "            ('categoricalPreprocessor', categorical_pipeline, cat_cols)\n",
    "        ])\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becfb5da6c8b4e28a55c1cdb39571170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "drop_cols = [\n",
    "    'date_recorded', \n",
    "    'recorded_by',\n",
    "    'num_private',\n",
    "    'public_meeting'\n",
    "]\n",
    "preprocessor = make_preprocessor(drop_cols=drop_cols, complexity_threshold=200, priority='information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipeline\n",
    "Below we build our main pipeline which executes two steps.\n",
    "1. Apply preprocessing to the raw data.\n",
    "2. Fit a one vs rest classifier to the processed data using an eXtreme Gradient Boosted forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('resampler', SMOTE(random_state=random_state)),\n",
    "        ('classifier', OneVsRestClassifier(estimator=XGBClassifier()))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('numericalPreprocessor',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('standardizer',\n",
       "                                                                   PowerTransformer())]),\n",
       "                                                  ['amount_tsh', 'gps_height',\n",
       "                                                   'longitude', 'latitude',\n",
       "                                                   'region_code',\n",
       "                                                   'district_code',\n",
       "                                                   'population',\n",
       "                                                   'construction_year']),\n",
       "                                                 ('categoricalPreprocessor',\n",
       "                                                  Pipeline(steps=[('ty...\n",
       "                                                   'subvillage', 'ward',\n",
       "                                                   'scheme_name', 'region',\n",
       "                                                   'extraction_type_group',\n",
       "                                                   'extraction_type_class',\n",
       "                                                   'extraction_type_class',\n",
       "                                                   'management_group',\n",
       "                                                   'payment_type',\n",
       "                                                   'quality_group',\n",
       "                                                   'quantity_group',\n",
       "                                                   'source_type',\n",
       "                                                   'source_class',\n",
       "                                                   'source_class',\n",
       "                                                   'waterpoint_type_group'])])),\n",
       "                ('resampler', SMOTE(random_state=42)),\n",
       "                ('classifier', OneVsRestClassifier(estimator=XGBClassifier()))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6914141414141414"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Parameter Grid\n",
    "Below we define a grid of hyper-parameters for our pipeline that will be tested in a grid search below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [\n",
    "    {\n",
    "        'classifier__estimator': [XGBClassifier()]\n",
    "        \n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Grid Search\n",
    "Below we instantiate a grid search object which will fit our pipeline for every combination of the parameters defined above. Since the competition uses accuracy as it's measure of model quality, we sill evaluate model performance in terms of accuracy. For each parameter combination, the grid search will also execute five-fold cross validation. \n",
    "\n",
    "In order to maximize performance, we will fit our grid search on the full provided training data set and select our best hyper-parameters based on the results of cross validation. For the purposes of local model evaluation, we will then refit the best model on our local training data and use our local testing data to produce a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_grid=parameter_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=5, \n",
    "    verbose=2, \n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Grid Search\n",
    "Below we fit our grid search on the full training set and select the best model hyper-parameters. This step takes an Extremely long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_search.fit(X, y)\n",
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Results of Grid Search\n",
    "Below we display the results of our grid search. We pay particular attention to `std_test_score` which will become larger if the model is over-fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions for Validation\n",
    "Below we import the testing data provided by the competition. To maximize performance we refit our model on the full training data set. Predictions are formatted and saved to CSV for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate = pd.read_csv('../data/testing_features.csv', index_col='id')\n",
    "y_validate = model.predict(X_validate)\n",
    "df_predictions = pd.DataFrame(y_validate, index=X_validate.index, columns=['status_group'])\n",
    "df_predictions.to_csv('../predictions/final_model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Confusion Matrix\n",
    "Below we fit the model on our local training data and produce a confusion matrix using the local test data. This provides a reasonable indication of how the model performs. Because the model needs to be fit before producing the matrix, this step will take a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "plot_confusion_matrix(model, X_test, y_test, ax=ax, normalize='true', include_values=True)\n",
    "fig.savefig('../images/Confusion_Matrix.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flatiron",
   "language": "python",
   "name": "flatiron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
